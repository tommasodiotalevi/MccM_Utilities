{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGF HH production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_fragment_CP5 = \"\"\"\n",
    "from Configuration.Generator.Pythia8CommonSettings_cfi import *\n",
    "from Configuration.Generator.MCTunes2017.PythiaCP5Settings_cfi import *\n",
    "\n",
    "generator = cms.EDFilter(\"Pythia8HadronizerFilter\",\n",
    "                         maxEventsToPrint = cms.untracked.int32(1),\n",
    "                         pythiaPylistVerbosity = cms.untracked.int32(1),\n",
    "                         filterEfficiency = cms.untracked.double(1.0),\n",
    "                         pythiaHepMCVerbosity = cms.untracked.bool(False),\n",
    "                         comEnergy = cms.double(13000.),\n",
    "                         PythiaParameters = cms.PSet(\n",
    "                                                     pythia8CommonSettingsBlock,\n",
    "                                                     pythia8CP5SettingsBlock,\n",
    "                                                     processParameters = cms.vstring(\n",
    "                                                     __CHANNEL_DECAY_FRAGMENT__\n",
    "                                                     ),\n",
    "                                                     parameterSets = cms.vstring('pythia8CommonSettings',\n",
    "                                                                                 'pythia8CP5Settings',\n",
    "                                                                                 'processParameters'\n",
    "                                                                                 )\n",
    "                                                     )\n",
    "                         )\n",
    "\n",
    "\n",
    "ProductionFilterSequence = cms.Sequence(generator)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragmentsDictCreator (decay_fr):\n",
    "    dict = { '2016': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2016APV': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2017': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2018': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr)\n",
    "                    }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHEproducer = \"\"\"\n",
    "import FWCore.ParameterSet.Config as cms\n",
    "\n",
    "# link to cards:\n",
    "# __EXAMPLE__\n",
    "\n",
    "externalLHEProducer = cms.EDProducer('ExternalLHEProducer',\n",
    "    args = cms.vstring('__GRIDPACK__'),\n",
    "    nEvents = cms.untracked.uint32(5000),\n",
    "    numberOfParameters = cms.uint32(1),\n",
    "    outputFile = cms.string('cmsgrid_final.lhe'),\n",
    "    scriptName = cms.FileInPath('GeneratorInterface/LHEInterface/data/run_generic_tarball_cvmfs.sh')\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2016\", \"2016APV\", \"2017\", \"2018\"]\n",
    "processes = [\"Radion\",\"BulkGraviton\"]\n",
    "mass_point = [\"250\",\"260\",\"270\",\"280\",\"300\",\"320\",\"350\",\"400\",\"450\",\"500\",\"550\",\"600\",\"650\",\"700\",\"750\",\"800\",\"850\",\"900\",\"1000\",\"1250\",\"1500\",\"1750\",\"2000\",\"2500\",\"3000\"]\n",
    "tot_events = [400000]*4 + [300000]*6 + [200000]*8 + [100000]*7\n",
    "gridpacks_dict = {}\n",
    "example_dict = {}\n",
    "dataset_names = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(len(years)):\n",
    "    t_example1 = 'https://github.com/cms-sw/genproductions/tree/8f5a3e8ffe6877d0a618eb0b9d8601c952cbf8f9/bin/MadGraph5_aMCatNLO/cards/production/2017/13TeV/exo_diboson/Spin-0/Radion_hh_narrow/Radion_hh_narrow_M900'\n",
    "    t_example2 = 'https://github.com/cms-sw/genproductions/tree/8f5a3e8ffe6877d0a618eb0b9d8601c952cbf8f9/bin/MadGraph5_aMCatNLO/cards/production/2017/13TeV/exo_diboson/Spin-2/cards_templates'\n",
    "    t_datasetname_year = 'GluGluTo{process}ToHHTo2B2Tau_M-{mass}_TuneCP5_narrow_13TeV-mcatnlo-pythia8'\n",
    "    t_gp_year = \"/cvmfs/cms.cern.ch/phys_generator/gridpacks/UL/13TeV/madgraph/V5_2.6.5/GF_Spin_{spin}/{process}_hh{graviton}narrow_M{mass}/v1/{process}_hh{graviton}narrow_M{mass}_slc7_amd64_gcc700_CMSSW_10_6_19_tarball.tar.xz\"    \n",
    "    tmp_dataset_dict={}\n",
    "    tmp_gridpack_dict={}\n",
    "    tmp_example_dict={}\n",
    "    dataset_names_year = []\n",
    "    gp_ggf_year = \"gp_ggf_\" + years[year]\n",
    "    gp_ggf_year = []\n",
    "    \n",
    "\n",
    "    for process in range(len(processes)):\n",
    "        #gp_ggf_year = \"gp_ggf_\" + years[year]\n",
    "        dataset_names_year = []\n",
    "        gp_ggf_year = []\n",
    "\n",
    "        for mass in mass_point:\n",
    "            if processes[process] == \"Radion\":\n",
    "                d = {\"process\":processes[process], \"mass\": mass, \"spin\": \"0\", \"graviton\": \"_\"}\n",
    "                tmp_example_dict[processes[process]] = t_example1\n",
    "            elif processes[process] == \"BulkGraviton\":\n",
    "                d = {\"process\":processes[process], \"mass\": mass, \"spin\": \"2\", \"graviton\": \"_GF_HH_\"}\n",
    "                tmp_example_dict[processes[process]] = t_example2\n",
    "            gp_ggf_year.append(t_gp_year.format_map(d))\n",
    "            dataset_names_year.append(t_datasetname_year.format_map(d))\n",
    "            tmp_dataset_dict[processes[process]] = dataset_names_year\n",
    "            tmp_gridpack_dict[processes[process]] = gp_ggf_year\n",
    "    gridpacks_dict[years[year]] = tmp_gridpack_dict\n",
    "    dataset_names[years[year]] = tmp_dataset_dict\n",
    "    example_dict[years[year]] = tmp_example_dict\n",
    "\n",
    "print(example_dict['2016']['Radion'])\n",
    "#gridpacks_dict = {'VBF_HH' : {'2016': gp_vbf_2016, '2017': gp_vbf_2017, '2018': gp_vbf_2018}}                                                                                                              \n",
    "#for x in gridpacks_dict:                                                                                                                                                                                   \n",
    "#    print (x)                                                                                                                                                                                              \n",
    "#    for y in gridpacks_dict[x]:                                                                                                                                                                            \n",
    "#        print (y,':', gridpacks_dict[x][y])                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggf_hh_bbtt = '''                                                                                                                                                                                           \n",
    "    '25:m0 = 125.0',\n",
    "    '25:onMode = off',\n",
    "    '25:onIfMatch = 5 -5',\n",
    "    '25:onIfMatch = 15 -15',\n",
    "    'ResonanceDecayFilter:filter = on',\n",
    "    'ResonanceDecayFilter:exclusive = on', #off: require at least the specified number of daughters, on: require exactly the specified number of daughters\n",
    "    'ResonanceDecayFilter:mothers = 25', #list of mothers not specified -> count all particles in hard process+resonance decays (better to avoid specifying mothers when including leptons from the lhe in counting, since intermediate resonances are not gauranteed to appear in general\n",
    "    'ResonanceDecayFilter:daughters = 5,5,15,15',\n",
    "'''\n",
    "#'SpaceShower:dipoleRecoil = on' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .csv Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MadgraphVersion = re.compile(\"V5_2\\.[0-9]\\.[0-9]\")\n",
    "process_pythia_map = {}\n",
    "tmp_process_pythia_map = {}\n",
    "for process in processes:\n",
    "    tmp_process_pythia_map[process] = fragmentsDictCreator(ggf_hh_bbtt)\n",
    "process_pythia_map.update(tmp_process_pythia_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in range(len(years)):\n",
    "for year in gridpacks_dict:\n",
    "    #print(year)\n",
    "    for process in processes:\n",
    "        with open('ggF_'+process+'_'+year+'.csv', 'w') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['Dataset name','Events', 'fragment','notes','Generator','mcdbid','time','size'])\n",
    "        \n",
    "            for i in range(len(mass_point)):\n",
    "                tmp_fragment=\"\"\n",
    "                #print(gridpacks_dict[year][i])\n",
    "                #print(dataset_names[year][i])\n",
    "                #version = MadgraphVersion.search(path).group(0)\n",
    "                version=\"2.6.5\"\n",
    "                dataset_name = dataset_names[year][process][i]\n",
    "                if year == \"2016APV\":\n",
    "                    events = round(tot_events[i]*0.54)\n",
    "                elif year == \"2016\":\n",
    "                    events = round(tot_events[i]*0.46)\n",
    "                else:\n",
    "                    events = tot_events[i]\n",
    "                tmp_fragment = LHEproducer.replace('__GRIDPACK__',gridpacks_dict[year][process][i]) + '\\n' + process_pythia_map[process][year]\n",
    "                final_fragment = tmp_fragment.replace('__EXAMPLE__',example_dict[year][process])\n",
    "                note = dataset_name.replace('_',' ')\n",
    "                generators=\"Madgraph_\" + version + \"  Pythia8\"\n",
    "                mcdb_id = '0'\n",
    "                time = '0.3'\n",
    "                size = '120'\n",
    "                csvwriter.writerow([dataset_name, events, final_fragment, note, generators, mcdb_id, time, size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
