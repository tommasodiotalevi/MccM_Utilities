{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGF HH production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_fragment_CP5 = \"\"\"\n",
    "from Configuration.Generator.Pythia8CommonSettings_cfi import *\n",
    "from Configuration.Generator.MCTunes2017.PythiaCP5Settings_cfi import *\n",
    "from Configuration.Generator.PSweightsPythia.PythiaPSweightsSettings_cfi import *\n",
    "\n",
    "generator = cms.EDFilter(\"Pythia8HadronizerFilter\",\n",
    "                         maxEventsToPrint = cms.untracked.int32(1),\n",
    "                         pythiaPylistVerbosity = cms.untracked.int32(1),\n",
    "                         filterEfficiency = cms.untracked.double(1.0),\n",
    "                         pythiaHepMCVerbosity = cms.untracked.bool(False),\n",
    "                         comEnergy = cms.double(13000.),\n",
    "                         PythiaParameters = cms.PSet(\n",
    "        pythia8CommonSettingsBlock,\n",
    "        pythia8CP5SettingsBlock,\n",
    "        pythia8PSweightsSettingsBlock,\n",
    "        processParameters = cms.vstring(__CHANNEL_DECAY_FRAGMENT__),\n",
    "        parameterSets = cms.vstring('pythia8CommonSettings',\n",
    "                                    'pythia8CP5Settings',\n",
    "                                    'processParameters',\n",
    "                                    'pythia8PSweightsSettings'\n",
    "                                    )\n",
    "        )\n",
    "                         )\n",
    "\n",
    "ProductionFilterSequence = cms.Sequence(generator)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragmentsDictCreator (decay_fr):\n",
    "    dict = { '2016': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2016APV': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2017': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2018': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr)\n",
    "                    }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHEproducer = \"\"\"\n",
    "import FWCore.ParameterSet.Config as cms\n",
    "\n",
    "# link to cards:\n",
    "# __EXAMPLE__\n",
    "\n",
    "externalLHEProducer = cms.EDProducer('ExternalLHEProducer',\n",
    "    args = cms.vstring('__GRIDPACK__'),\n",
    "    nEvents = cms.untracked.uint32(5000),\n",
    "    numberOfParameters = cms.uint32(1),\n",
    "    outputFile = cms.string('cmsgrid_final.lhe'),\n",
    "    scriptName = cms.FileInPath('GeneratorInterface/LHEInterface/data/run_generic_tarball_cvmfs.sh')\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2016\", \"2016APV\", \"2017\", \"2018\"]\n",
    "processes = [\"Radion\",\"BulkGraviton\"]\n",
    "decays = [\"2G2WTo2G2Q1L1Nu\",\"2G2WTo2G2l2Nu\",\"2G2WTo2G4Q\",\"2G2ZTo2G4Q\"]\n",
    "mass_point = [\"250\",\"260\",\"270\",\"280\",\"300\",\"320\",\"350\",\"400\",\"450\",\"500\",\"550\",\"600\",\"650\",\"700\",\"750\",\"800\",\"850\",\"900\",\"1000\"]\n",
    "tot_events = [800000]*19\n",
    "gridpacks_dict = {}\n",
    "example_dict = {}\n",
    "dataset_names = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(len(years)):\n",
    "    t_example1 = 'https://github.com/cms-sw/genproductions/tree/master/bin/MadGraph5_aMCatNLO/cards/production/2017/13TeV/exo_diboson/Spin-0'\n",
    "    t_example2 = 'https://github.com/cms-sw/genproductions/tree/master/bin/MadGraph5_aMCatNLO/cards/production/2017/13TeV/exo_diboson/Spin-2'\n",
    "    t_datasetname_year = 'GluGluTo{process}ToHHTo{decay}_M-{mass}_TuneCP5_PSWeights_narrow_13TeV-madgraph-pythia8'\n",
    "    t_gp_year = \"/cvmfs/cms.cern.ch/phys_generator/gridpacks/UL/13TeV/madgraph/V5_2.6.5/GF_Spin_{spin}/{process}_hh{graviton}narrow_M{mass}/v1/{process}_hh{graviton}narrow_M{mass}_slc7_amd64_gcc700_CMSSW_10_6_19_tarball.tar.xz\"    \n",
    "    tmp_dataset_dict={}\n",
    "    tmp_gridpack_dict={}\n",
    "    tmp_example_dict={}\n",
    "    dataset_names_year = []\n",
    "    gp_ggf_year = \"gp_ggf_\" + years[year]\n",
    "    gp_ggf_year = []\n",
    "    \n",
    "\n",
    "    for process in range(len(processes)):\n",
    "        #gp_ggf_year = \"gp_ggf_\" + years[year]\n",
    "        dataset_names_year = []\n",
    "        gp_ggf_year = []\n",
    "        tmp_example_dict[processes[process]]={}\n",
    "        tmp_dataset_dict[processes[process]]={}\n",
    "        tmp_gridpack_dict[processes[process]]={}\n",
    "        for decay in decays:\n",
    "            dataset_names_year = []\n",
    "            gp_ggf_year = []\n",
    "            for mass in mass_point:\n",
    "                if processes[process] == \"Radion\":\n",
    "                    d = {\"process\":processes[process], \"decay\": decay, \"mass\": mass, \"spin\": \"0\", \"graviton\": \"_\"}\n",
    "                    tmp_example_dict[processes[process]][decay] = t_example1\n",
    "                elif processes[process] == \"BulkGraviton\":\n",
    "                    d = {\"process\":processes[process], \"decay\": decay, \"mass\": mass, \"spin\": \"2\", \"graviton\": \"_GF_HH_\"}\n",
    "                    tmp_example_dict[processes[process]][decay] = t_example2\n",
    "                gp_ggf_year.append(t_gp_year.format_map(d))\n",
    "                dataset_names_year.append(t_datasetname_year.format_map(d))\n",
    "                tmp_dataset_dict[processes[process]][decay] = dataset_names_year\n",
    "                tmp_gridpack_dict[processes[process]][decay] = gp_ggf_year\n",
    "    gridpacks_dict[years[year]] = tmp_gridpack_dict\n",
    "    dataset_names[years[year]] = tmp_dataset_dict\n",
    "    example_dict[years[year]] = tmp_example_dict\n",
    "#print(example_dict['2016']['Radion'])\n",
    "#gridpacks_dict = {'VBF_HH' : {'2016': gp_vbf_2016, '2017': gp_vbf_2017, '2018': gp_vbf_2018}}                                                                                                              \n",
    "#for x in gridpacks_dict:                                                                                                                                                                                   \n",
    "#    print (x)                                                                                                                                                                                              \n",
    "#    for y in gridpacks_dict[x]:                                                                                                                                                                            \n",
    "#        print (y,':', gridpacks_dict[x][y])                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggf_hh_2G2WTo2G2Q1L1Nu = '''                                                                                                                                                                                           \n",
    "    '15:onMode = on', # allow all tau decays. Leptonic and Hadronic \n",
    "    '24:mMin = 0.05', # the lower limit of the allowed mass range generated by the Breit-Wigner (in GeV)\n",
    "    '24:onMode = off', # Turn off all W decays \n",
    "    '24:onIfAny = 1 2 3 4 5 11 13 15', # Add W->enu, W->munu, W->taunu. Add W->qq decays \n",
    "    '25:onMode = off', # Turn off all H decays \n",
    "    '25:onIfMatch = 22 22', # Add H->gg decay\n",
    "    '25:onIfMatch = 24 -24', # Add H->WW decay\n",
    "    'ResonanceDecayFilter:filter = on',\n",
    "    'ResonanceDecayFilter:exclusive = on', #off: require at least the specified number of daughters, on: require exactly the specified number of daughters\n",
    "    'ResonanceDecayFilter:eMuTauAsEquivalent = on', #on: treat electrons, muons , and taus as equivalent\n",
    "    'ResonanceDecayFilter:allNuAsEquivalent  = on', #on: treat all three neutrino flavours as equivalent\n",
    "    'ResonanceDecayFilter:udscbAsEquivalent  = on',  #on: treat udscb quarks as equivalent\n",
    "    'ResonanceDecayFilter:mothers = 25,24', #list of mothers not specified -> count all particles in hard process+resonance decays (better to avoid specifying mothers when including leptons from the lhe in counting, since intermediate resonances are not gauranteed to appear in general\n",
    "    'ResonanceDecayFilter:daughters = 1,1,11,12,22,22', # qq,lnu,gg\n",
    "'''\n",
    "\n",
    "ggf_hh_2G2WTo2G2l2Nu = '''\n",
    "    '15:onMode = on', # allow all tau decays. Leptonic and Hadronic \n",
    "    '24:mMin = 0.05', # the lower limit of the allowed mass range generated by the Breit-Wigner (in GeV)\n",
    "    '24:onMode = off', # Turn off all W decays \n",
    "    '24:onIfAny = 11 13 15', # Add W->enu, W->munu, W->taunu \n",
    "    '25:onMode = off', # Turn off all H decays \n",
    "    '25:onIfMatch = 22 22', # Add H->gg decay\n",
    "    '25:onIfMatch = 24 -24', # Add H->WW decay\n",
    "    'ResonanceDecayFilter:filter = on',\n",
    "    'ResonanceDecayFilter:exclusive = on', #off: require at least the specified number of daughters, on: require exactly the specified number of daughters\n",
    "    'ResonanceDecayFilter:eMuTauAsEquivalent = on', #on: treat electrons, muons , and taus as equivalent\n",
    "    'ResonanceDecayFilter:allNuAsEquivalent  = on', #on: treat all three neutrino flavours as equivalent\n",
    "    'ResonanceDecayFilter:udscbAsEquivalent  = on',  #on: treat udscb quarks as equivalent\n",
    "    'ResonanceDecayFilter:mothers = 25,24', #list of mothers not specified -> count all particles in hard process+resonance decays (better to avoid specifying mothers when including leptons from the lhe in counting, since intermediate resonances are not gauranteed to appear in general\n",
    "    'ResonanceDecayFilter:daughters = 11,12,11,12,22,22', # lnu,lnu,gg \n",
    "'''\n",
    "\n",
    "ggf_hh_2G2WTo2G4Q = '''\n",
    "    '24:mMin = 0.05', # the lower limit of the allowed mass range generated by the Breit-Wigner (in GeV)\n",
    "    '24:onMode = off', # Turn off all W decays \n",
    "    '24:onIfAny = 1 2 3 4 5', # Add W->qq decays \n",
    "    '25:onMode = off', # Turn off all H decays \n",
    "    '25:onIfMatch = 22 22', # Add H->gg decay\n",
    "    '25:onIfMatch = 24 -24', # Add H->WW decay \n",
    "    'ResonanceDecayFilter:filter = on',\n",
    "    'ResonanceDecayFilter:exclusive = on', #off: require at least the specified number of daughters, on: require exactly the specified number of daughters\n",
    "    'ResonanceDecayFilter:udscbAsEquivalent  = on',  #on: treat udscb quarks as equivalent\n",
    "    'ResonanceDecayFilter:mothers = 25,24', #list of mothers not specified -> count all particles in hard process+resonance decays (better to avoid specifying mothers when including leptons from the lhe in counting, since intermediate resonances are not gauranteed to appear in general\n",
    "    'ResonanceDecayFilter:daughters = 1,1,1,1,22,22', # qq,qq,gg \n",
    "'''\n",
    "\n",
    "ggf_hh_2G2ZTo2G4Q = '''\n",
    "    '23:mMin = 0.05', # the lower limit of the allowed mass range generated by the Breit-Wigner (in GeV)\n",
    "    '23:onMode = off', # Turn off all Z decays \n",
    "    '23:onIfAny = 1 2 3 4 5', # Add Z->qq decays \n",
    "    '25:onMode = off', # Turn off all H decays \n",
    "    '25:onIfMatch = 22 22', # Add H->gg decay\n",
    "    '25:onIfMatch = 23 23', # Add H->ZZ decay \n",
    "    'ResonanceDecayFilter:filter = on',\n",
    "    'ResonanceDecayFilter:exclusive = on', #off: require at least the specified number of daughters, on: require exactly the specified number of daughters\n",
    "    'ResonanceDecayFilter:udscbAsEquivalent  = on',  #on: treat udscb quarks as equivalent\n",
    "    'ResonanceDecayFilter:mothers = 25,23', #list of mothers not specified -> count all particles in hard process+resonance decays (better to avoid specifying mothers when including leptons from the lhe in counting, since intermediate resonances are not gauranteed to appear in general\n",
    "    'ResonanceDecayFilter:daughters = 1,1,1,1,22,22', # qq,qq,gg \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .csv Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MadgraphVersion = re.compile(\"V5_2\\.[0-9]\\.[0-9]\")\n",
    "process_pythia_map = {}\n",
    "tmp_process_pythia_map = {}\n",
    "for process in processes:\n",
    "    tmp_process_pythia_map[process] = {}\n",
    "    for decay in decays:\n",
    "        if decay == \"2G2WTo2G2Q1L1Nu\":\n",
    "            tmp_process_pythia_map[process][decay] = fragmentsDictCreator(ggf_hh_2G2WTo2G2Q1L1Nu)\n",
    "        elif decay == \"2G2WTo2G2l2Nu\":\n",
    "            tmp_process_pythia_map[process][decay] = fragmentsDictCreator(ggf_hh_2G2WTo2G2l2Nu)\n",
    "        elif decay == \"2G2WTo2G4Q\":\n",
    "            tmp_process_pythia_map[process][decay] = fragmentsDictCreator(ggf_hh_2G2WTo2G4Q)\n",
    "        elif decay == \"2G2ZTo2G4Q\":\n",
    "            tmp_process_pythia_map[process][decay] = fragmentsDictCreator(ggf_hh_2G2ZTo2G4Q)\n",
    "process_pythia_map.update(tmp_process_pythia_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in range(len(years)):\n",
    "decay_mode = [\"SL\",\"FL\",\"FH\",\"FHZZ\"]\n",
    "for year in gridpacks_dict:\n",
    "    #print(year)\n",
    "    for process in processes:\n",
    "        for dm in decay_mode:\n",
    "            with open('ggF_'+ process + '_' + dm + '_'+year+'.csv', 'w') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow(['Dataset name','Events', 'fragment','notes','Generator','mcdbid','time','size'])\n",
    "                for i in range(len(mass_point)):\n",
    "                    tmp_fragment=\"\"\n",
    "                    #print(gridpacks_dict[year][i])\n",
    "                    #print(dataset_names[year][i])\n",
    "                    #version = MadgraphVersion.search(path).group(0)\n",
    "                    version=\"2.6.5\"\n",
    "                    if year == \"2016APV\":\n",
    "                        events = round(tot_events[i]*0.54)\n",
    "                    elif year == \"2016\":\n",
    "                        events = round(tot_events[i]*0.46)\n",
    "                    elif year == \"2017\":\n",
    "                        events = tot_events[i]\n",
    "                    elif year == \"2018\":\n",
    "                        events = tot_events[i]\n",
    "                        \n",
    "                    if dm == \"SL\":\n",
    "                        dataset_name = dataset_names[year][process][\"2G2WTo2G2Q1L1Nu\"][i]\n",
    "                        tmp_fragment = LHEproducer.replace('__GRIDPACK__',gridpacks_dict[year][process][\"2G2WTo2G2Q1L1Nu\"][i]) + '\\n' + process_pythia_map[process][\"2G2WTo2G2Q1L1Nu\"][year]\n",
    "                        note = dataset_name.replace('_',' ')\n",
    "                    elif dm == \"FL\":\n",
    "                        dataset_name = dataset_names[year][process][\"2G2WTo2G2l2Nu\"][i]\n",
    "                        tmp_fragment = LHEproducer.replace('__GRIDPACK__',gridpacks_dict[year][process][\"2G2WTo2G2l2Nu\"][i]) + '\\n' + process_pythia_map[process][\"2G2WTo2G2l2Nu\"][year]\n",
    "                        note = dataset_name.replace('_',' ')\n",
    "                    elif dm == \"FH\":\n",
    "                        dataset_name = dataset_names[year][process][\"2G2WTo2G4Q\"][i]\n",
    "                        tmp_fragment = LHEproducer.replace('__GRIDPACK__',gridpacks_dict[year][process][\"2G2WTo2G4Q\"][i]) + '\\n' + process_pythia_map[process][\"2G2WTo2G4Q\"][year]\n",
    "                        note = dataset_name.replace('_',' ')\n",
    "                    elif dm == \"FHZZ\":\n",
    "                        dataset_name = dataset_names[year][process][\"2G2ZTo2G4Q\"][i]\n",
    "                        tmp_fragment = LHEproducer.replace('__GRIDPACK__',gridpacks_dict[year][process][\"2G2ZTo2G4Q\"][i]) + '\\n' + process_pythia_map[process][\"2G2ZTo2G4Q\"][year]\n",
    "                        note = dataset_name.replace('_',' ')\n",
    "                    final_fragment = tmp_fragment.replace('__EXAMPLE__',example_dict[year][process][\"2G2WTo2G2Q1L1Nu\"])\n",
    "                    generators=\"Madgraph_\" + version + \"  Pythia8\"\n",
    "                    mcdb_id = '0'\n",
    "                    time = '0.001'\n",
    "                    size = '30'\n",
    "                    csvwriter.writerow([dataset_name, events, final_fragment, note, generators, mcdb_id, time, size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
