{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGF HH production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_fragment_CP5 = \"\"\"\n",
    "from Configuration.Generator.Pythia8CommonSettings_cfi import *\n",
    "from Configuration.Generator.MCTunesRun3ECM13p6TeV.PythiaCP5Settings_cfi import *\n",
    "from Configuration.Generator.PSweightsPythia.PythiaPSweightsSettings_cfi import *\n",
    "\n",
    "generator = cms.EDFilter(\"Pythia8ConcurrentHadronizerFilter\",\n",
    "                         maxEventsToPrint = cms.untracked.int32(1),\n",
    "                         pythiaPylistVerbosity = cms.untracked.int32(1),\n",
    "                         filterEfficiency = cms.untracked.double(1.0),\n",
    "                         pythiaHepMCVerbosity = cms.untracked.bool(False),\n",
    "                         comEnergy = cms.double(13600.),\n",
    "                         PythiaParameters = cms.PSet(\n",
    "        pythia8CommonSettingsBlock,\n",
    "        pythia8CP5SettingsBlock,\n",
    "        pythia8PSweightsSettingsBlock,\n",
    "        processParameters = cms.vstring(\n",
    "        __CHANNEL_DECAY_FRAGMENT__\n",
    "            ),\n",
    "        parameterSets = cms.vstring('pythia8CommonSettings',\n",
    "                                    'pythia8CP5Settings',\n",
    "                                    'pythia8PSweightsSettings',\n",
    "                                    'processParameters'\n",
    "                                    )\n",
    "        )\n",
    "                         )\n",
    "\n",
    "ProductionFilterSequence = cms.Sequence(generator)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragmentsDictCreator (decay_fr):\n",
    "    dict = { '2022': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "             '2022EE': pythia_fragment_CP5.replace('__CHANNEL_DECAY_FRAGMENT__',decay_fr),\n",
    "                    }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHEproducer = \"\"\"\n",
    "import FWCore.ParameterSet.Config as cms\n",
    "\n",
    "# link to cards:\n",
    "# __EXAMPLE__\n",
    "\n",
    "externalLHEProducer = cms.EDProducer('ExternalLHEProducer',\n",
    "    args = cms.vstring('__GRIDPACK__'),\n",
    "    nEvents = cms.untracked.uint32(5000),\n",
    "    numberOfParameters = cms.uint32(1),\n",
    "    outputFile = cms.string('cmsgrid_final.lhe'),\n",
    "    generateConcurrently = cms.untracked.bool(False),\n",
    "    scriptName = cms.FileInPath('GeneratorInterface/LHEInterface/data/run_generic_tarball_cvmfs.sh')\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2022\",\"2022EE\"]\n",
    "processes = [\"Radion\", \"BulkGraviton\"]\n",
    "mass_point = [\"250\", \"260\", \"270\", \"280\", \"300\", \"350\", \"450\", \"550\", \"600\", \"650\", \"700\", \"800\", \"1000\",\n",
    "\"1200\", \"1400\", \"1600\", \"1800\", \"2000\", \"2500\", \"3000\", \"4000\", \"5000\"]\n",
    "tot_events = [400000]*13+[100000]*9\n",
    "gridpacks_dict = {}\n",
    "example_dict = {}\n",
    "dataset_names = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(len(years)):\n",
    "    t_example1 = 'https://github.com/cms-sw/genproductions/tree/master/bin/MadGraph5_aMCatNLO/cards/production/13p6TeV/HHresonant/Spin-0'\n",
    "    t_example2 = 'https://github.com/cms-sw/genproductions/tree/master/bin/MadGraph5_aMCatNLO/cards/production/13p6TeV/HHresonant/Spin-2'\n",
    "    t_datasetname_year = 'GluGluto{process}toHHto4Tau_M-{mass}_narrow_TuneCP5_13p6TeV_madgraph-pythia8'\n",
    "    t_gp_year = \"/cvmfs/cms.cern.ch/phys_generator/gridpacks/RunIII/13p6TeV/slc7_amd64_gcc10/MadGraph5_aMCatNLO/GF_HH_Spin{spin}/{process}_hh{graviton}narrow_M{mass}_slc7_amd64_gcc10_CMSSW_12_4_8_tarball.tar.xz\"    \n",
    "    tmp_dataset_dict={}\n",
    "    tmp_gridpack_dict={}\n",
    "    tmp_example_dict={}\n",
    "    dataset_names_year = []\n",
    "    gp_ggf_year = \"gp_ggf_\" + years[year]\n",
    "    gp_ggf_year = []\n",
    "    \n",
    "\n",
    "    for process in range(len(processes)):\n",
    "        #gp_ggf_year = \"gp_ggf_\" + years[year]\n",
    "        dataset_names_year = []\n",
    "        gp_ggf_year = []\n",
    "\n",
    "        for mass in mass_point:\n",
    "            if processes[process] == \"Radion\":\n",
    "                d = {\"process\":processes[process], \"mass\": mass, \"spin\": \"0\", \"graviton\": \"_\"}\n",
    "                tmp_example_dict[processes[process]] = t_example1\n",
    "            elif processes[process] == \"BulkGraviton\":\n",
    "                d = {\"process\":processes[process], \"mass\": mass, \"spin\": \"2\", \"graviton\": \"_GF_HH_\"}\n",
    "                tmp_example_dict[processes[process]] = t_example2\n",
    "            gp_ggf_year.append(t_gp_year.format_map(d))\n",
    "            dataset_names_year.append(t_datasetname_year.format_map(d))\n",
    "            tmp_dataset_dict[processes[process]] = dataset_names_year\n",
    "            tmp_gridpack_dict[processes[process]] = gp_ggf_year\n",
    "    gridpacks_dict[years[year]] = tmp_gridpack_dict\n",
    "    dataset_names[years[year]] = tmp_dataset_dict\n",
    "    example_dict[years[year]] = tmp_example_dict\n",
    "\n",
    "print(example_dict['2022']['Radion'])\n",
    "#gridpacks_dict = {'VBF_HH' : {'2016': gp_vbf_2016, '2017': gp_vbf_2017, '2018': gp_vbf_2018}}                                                                                                              \n",
    "#for x in gridpacks_dict:                                                                                                                                                                                   \n",
    "#    print (x)                                                                                                                                                                                              \n",
    "#    for y in gridpacks_dict[x]:                                                                                                                                                                            \n",
    "#        print (y,':', gridpacks_dict[x][y])                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggf_hh_tttt = '''                                                                                                                                                                                           \n",
    "    '23:mMin = 0.05',\n",
    "    '24:mMin = 0.05',\n",
    "    '25:m0 = 125.0',\n",
    "    '25:onMode = off',\n",
    "    '25:onIfMatch = 15 -15',\n",
    "    'ResonanceDecayFilter:filter = on',\n",
    "    'ResonanceDecayFilter:exclusive = on', #off: require at least the specified number of daughters, on: require exactly the specified number of daughters\n",
    "    'ResonanceDecayFilter:mothers = 25', #list of mothers not specified -> count all particles in hard process+resonance decays (better to avoid specifying mothers when including leptons from the lhe in counting, since intermediate resonances are not gauranteed to appear in general\n",
    "    'ResonanceDecayFilter:daughters = 15,15,15,15',\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .csv Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MadgraphVersion = re.compile(\"V5_2\\.[0-9]\\.[0-9]\")\n",
    "process_pythia_map = {}\n",
    "tmp_process_pythia_map = {}\n",
    "for process in processes:\n",
    "    tmp_process_pythia_map[process] = fragmentsDictCreator(ggf_hh_tttt)\n",
    "process_pythia_map.update(tmp_process_pythia_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in range(len(years)):\n",
    "for year in gridpacks_dict:\n",
    "    #print(year)\n",
    "    for process in processes:\n",
    "        with open('ggF_'+process+'_'+year+'.csv', 'w') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['Dataset name','Events', 'fragment','notes','Generator','mcdbid','time','size'])\n",
    "            '''if year == \"2016\":\n",
    "                if process == \"Radion\":\n",
    "                    pr = 1606\n",
    "                elif process == \"BulkGraviton\":\n",
    "                    pr = 1631\n",
    "            if year == \"2016APV\":\n",
    "                if process == \"Radion\":\n",
    "                    pr = 812\n",
    "                elif process == \"BulkGraviton\":\n",
    "                    pr = 837\n",
    "            if year == \"2017\":\n",
    "                if process == \"Radion\":\n",
    "                    pr = 1574\n",
    "                elif process == \"BulkGraviton\":\n",
    "                    pr = 1599\n",
    "            if year == \"2018\":\n",
    "                if process == \"Radion\":\n",
    "                    pr = 1612\n",
    "                elif process == \"BulkGraviton\":\n",
    "                    pr = 1637'''\n",
    "            for i in range(len(mass_point)):\n",
    "                tmp_fragment=\"\"\n",
    "                #print(gridpacks_dict[year][i])\n",
    "                #print(dataset_names[year][i])\n",
    "                #version = MadgraphVersion.search(path).group(0)\n",
    "                version=\"2.9.13\"\n",
    "                dataset_name = dataset_names[year][process][i]\n",
    "                if year == \"2022\":\n",
    "                    events = round(tot_events[i]*0.22)\n",
    "                    #prepid = 'HIG-RunIISummer20UL16wmLHEGENAPV-00' + str(pr)\n",
    "                elif year == \"2022EE\":\n",
    "                    events = round(tot_events[i]*0.78)\n",
    "                    #prepid = 'HIG-RunIISummer20UL16wmLHEGEN-0' + str(pr)\n",
    "                    \n",
    "                tmp_fragment = LHEproducer.replace('__GRIDPACK__',gridpacks_dict[year][process][i]) + '\\n' + process_pythia_map[process][year]\n",
    "                final_fragment = tmp_fragment.replace('__EXAMPLE__',example_dict[year][process])\n",
    "                note = dataset_name.replace('_',' ')\n",
    "                generators=\"Madgraph_\" + version + \"  Pythia8\"\n",
    "                mcdb_id = '0'\n",
    "                time = '38'\n",
    "                size = '870'\n",
    "                csvwriter.writerow([dataset_name, events, final_fragment, note, generators, mcdb_id, time, size])\n",
    "                #pr = pr+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
